\documentclass[12pt, fullpage,letterpaper]{article}

\usepackage[margin=1in]{geometry}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xspace}
\usepackage{graphicx}

\newcommand{\semester}{Spring 2019}
\newcommand{\assignmentId}{0}
\newcommand{\releaseDate}{7 January, 2019}
\newcommand{\dueDate}{11:59pm, 16 January, 2019}

\newcommand{\bx}{{\bf x}}
\newcommand{\bw}{{\bf w}}

\title{CS 5350/6350: Machine Learining \semester}
\author{Homework \assignmentId}
\date{Handed out: \releaseDate\\
  Due: \dueDate}

\begin{document}
\maketitle

\input{emacscomm}
\footnotesize
	\begin{itemize}
		\item You are welcome to talk to other members of the class about
		the homework. I am more concerned that you understand the
		underlying concepts. However, you should write down your own
		solution. Please keep the class collaboration policy in mind.
		
		\item Feel free discuss the homework with the instructor or the TAs.
		
		\item Your written solutions should be brief and clear. You need to
		show your work, not just the final answer, but you do \emph{not}
		need to write it in gory detail. Your assignment should be {\bf no
			more than 10 pages}. Every extra page will cost a point.
		
		\item Handwritten solutions will not be accepted.
		
		\item The homework is due by \textbf{midnight of the due date}. Please submit
		the homework on Canvas.
		
		\item Some questions are marked {\bf For 6350 students}. Students
		who are registered for CS 6350 should do these questions. Of
		course, if you are registered for CS 5350, you are welcome to do
		the question too, but you will not get any credit for it.
		
	\end{itemize}



\section*{Basic Knowledge Review}
\label{sec:q1}

%problem 1, decide wether dependency & indendency
%2, prove p(A + B) <= P(A) + P(B)
%3, prove P(\sum_i A_i) \le \sum_i p(A_i)
%4. given two a joint Gaussian, calculate the conditional Guassian distribution
%5. prove E(X) = E(E(X|Y))
%4, prove V(E(X)) = E(V(X))
%5. prove V(Y) = EV(Y|X) + VE(Y|X)

%independency, conditional distribution, expectation, variance, basic properties
%gradient calcualtion, logistic function, second derivatives
%
\begin{enumerate}
\item~[5 points] We use sets to represent events. For example, toss a fair coin $10$ times, and the event can be represented by the set of ``Heads" or ``Tails" after each tossing. Let a specific event $A$ be ``at least one head". Calculate the probability that event $A$ happens, i.e., $p(A)$.

\textbf{Solution:} We can compute this probability by just calculating the chance of all coin tosses being tails, then subtracting this from 1. The chance of 10 tails is just $(\frac{1}{2})^{10} = \frac{1}{1024}$, so $p(A) = 1 - \frac{1}{1024} = \frac{1023}{1024}$.

\item~[10 points] Given two events $A$ and $B$, prove that 
\[
p(A \cup B) \le p(A) + p(B).
\]
When does the equality hold?

\textbf{Solution:} This equality only holds when the two events $A$ and $B$ are mutually exclusive, that is, both events cannot occur simultaneously. This means that $p(A \cap B) = 0$. The probability of either event occurring is $p(A \cup B) = p(A) + p(B) - p(A \cap B)$, and because $p(A \cap B) = 0$ then we know that $p(A \cup B) = p(A) + p(B)$, satisfying the equality.

\item~[10 points] Let $\{A_1, \ldots, A_n\}$ be a collection of events. Show that
\[
p(\cup_{i=1}^n A_i) \le \sum_{i=1}^n p(A_i).
\]
When does the equality hold? (Hint: induction)

\textbf{Solution:} This equality also only holds when all events are mutually exclusive, that is for any two events the probability of both happening is 0. If this is the case, then for $n=2$ we have already proved the equality to hold in problem 2. Assume that the equality holds for $n=k$, then for $n=k+1$ there is an additional event $A_{k+1}$ and this event is also mutually exclusive for all other events, that is $p(A_{k+1} \cap (\cup^k_{i=1} A_i)) = 0$ . Because the equality holds for $n=k$, then we know that $p(\cup^k_{i=1} A_i) = \sum^k_{i=1} p(A_i)$, and $p(\cup^{k+1}_{i=1} A_i) = \sum^k_{i=1} p(A_i) - p(A_{k+1} \cap (\cup^k_{i=1} A_i))$ which satisfies the equality since all events are presumed to be mutually exclusive. Since this is true for $n=k+1$, then it must be true for all $n$.

%\item~[5 points] Given three events $A$, $B$ and $C$, show that
%\[
%p(A\cap B\cap C) = p(A|B\cap C)p(B|C)p(C)
%\]
\item~[20 points]  We use $\EE(\cdot)$ and $\VV(\cdot)$ to denote a random variable's mean (or expectation) and variance, respectively. Given two discrete random variables $X$ and $Y$, where $X \in \{0, 1\}$ and $Y \in \{0,1\}$. The joint probability $p(X,Y)$ is given in as follows:
\begin{table}[h]
        \centering
        \begin{tabular}{ccc}
        \hline\hline
         & $Y=0$ & $Y=1$ \\ \hline
         $X=0$ & $1/10$ & $2/10$ \\ \hline
         $X=1$  & $3/10$ & $4/10$ \\ \hline\hline
        \end{tabular}
        %\caption{Training data for the alien invasion problem.}\label{tb-alien-train}
        \end{table}
	
        \begin{enumerate}
            \item~[10 points] Calculate the following distributions and statistics. 
            \begin{enumerate}
            \item the the marginal distributions $p(X)$ and $p(Y)$
            \item the conditional distributions $p(X|Y)$ and $p(Y|X)$
            \item $\EE(X)$, $\EE(Y)$, $\VV(X)$, $\VV(Y)$
            \item  $\EE(Y|X=0)$, $\EE(Y|X=1)$,  $\VV(Y|X=0)$, $\VV(Y|X=1)$ 
            \item  the covariance between $X$ and $Y$
            \end{enumerate}
            \item~[5 points] Are $X$ and $Y$ independent? Why?
            \item~[5 points] When $X$ is not assigned a specific value, are $\EE(Y|X)$ and $\VV(Y|X)$ still constant? Why?
        \end{enumerate}
        
\textbf{Solution:}
\begin{enumerate}
\item 
	\begin{enumerate}
		\item $p(X = 0) = \frac{3}{10}$, $p(X=1) = \frac{7}{10}$ , $p(Y=0) = \frac{4}{10}$, $p(Y=1) = \frac{6}{10}$.
		\item $p(X = 0 | Y = 0) = 25\%$, $p(X = 1 | Y = 0) = 75\%$ \\$p(X = 0 | Y = 1) = 33.33\%$, $p(X = 1 | Y = 1) = 66.67\%$
		\item $\EE(X) = \frac{7}{10}$, $\EE(Y) = \frac{6}{10}$ \\ 
		$\VV(X) = \frac{7}{10} - (\frac{7}{10})^2 = 0.21$, $\VV(Y) = \frac{6}{10} - (\frac{6}{10})^2 = 0.24$
		\item $\EE(Y | X = 0) = \frac{2}{3}$, $\EE(Y | X = 1) = \frac{4}{7}$ \\ 
		$\VV(Y | X = 0) = \frac{2}{3} - (\frac{2}{3})^2 = \frac{2}{9}$, $\VV(Y | X = 1) = \frac{4}{7} - (\frac{4}{7})^2 = \frac{12}{49}$
		\item $Cov(X, Y) = (1)(1)\frac{4}{10} - \frac{7}{10}\frac{6}{10} = -\frac{2}{100}$
	\end{enumerate}
	\item No, their covariance is not 0.
	\item Yes, the expectation and variance in this scenario is equivalent to the expectation and variance of the marginal distribution $p(Y)$ because even though $X$ is not assigned a specific value, we know that some $X$ must occur, either a 0 or a 1.
\end{enumerate}
        
\item~[10 points] Assume a random variable $X$ follows a standard normal distribution, \ie $X \sim \N(X|0, 1)$. Let $Y = e^X$. Calculate the mean and variance of $Y$.
\begin{enumerate}
	\item $\EE(Y)$
	\item $\VV(Y)$
\end{enumerate}

\textbf{Solution:} The PDF $p(x)$ for this normal distribution is $\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}x^2}$:
\begin{enumerate}
	\item $\EE(Y) = \int_{-\infty}^{\infty} e^x p(x) = e^{\frac{1}{2}}$ (computed in MATLAB)
	\item $\VV(Y) = \int_{-\infty}^{\infty} (e^{x})^2 p(x) - \EE(Y)^2= e^2 - e^1$ (computed in MATLAB)
\end{enumerate}

\item~[20 points]  Given two random variables $X$ and $Y$, show that 
\begin{enumerate}
\item $\EE(\EE(Y|X)) = \EE(Y)$
\item
$\VV(Y) = \EE(\VV(Y|X)) + \VV(\EE(Y|X))$
\end{enumerate}
(Hints: using definition.)

\textbf{Solution:}
\begin{enumerate}
	\item Definition of expectation then reduction:
		$$\EE(\EE(Y | X)) = \int_{-\infty}^{\infty} \EE(Y | X = x)f_X(x) dx =$$
	 	$$\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} yf_{Y|X}(y|x)f_X(x) dy dx =$$
	 	$$\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} yf_{X,Y}(x,y) dx dy =$$
	 	$$\int_{-\infty}^{\infty} yf_Y(y) dy = \EE(Y)$$
	\item Definition of variance is:
		$$\VV(Y) = \EE(Y^2) - \EE(Y)^2 \\$$
		Using the equality found in part a: \\
		$$\VV(Y) = \EE(\EE(Y^2 | X)) - (\EE(\EE(Y | X)))^2 =$$
		$$\EE(\VV(Y | X) + (\EE(Y | X))^2) - (\EE(\EE(Y | X)))^2$$
		The terms $(\EE(Y | X))^2) - (\EE(\EE(Y | X)))^2$ is the same as $\VV(\EE(Y | X))$, thus: \\
		$$\VV(Y) = \EE(\VV(Y | X) + \VV(\EE(Y | X)$$
		
		
\end{enumerate}

%\item~[20 points]  Let us go back to the coin tossing example. Suppose we toss a coin for $n$ times, \textit{independently}. Each toss we have $\frac{1}{2}$ chance to obtain the head. Let us denote the total number of heads by $c(n)$. Derive the following statistics. You don't need to give the numerical values. You only need to provide the formula.
%\begin{enumerate}
%\item $\EE(c(1))$, $\VV(c(1))$
%\item $\EE(c(10))$, $\VV(c(10))$
%\item $\EE(c(n))$, $\VV(c(n))$
%\end{enumerate} 
%What can you conclude from comparing the expectations and variances with different choices of $n$?  

\item~[15 points] Given a logistic function, $f(\x) = 1/(1+\exp(-\a^\top \x))$ ($\x$ is a vector), derive/calculate the following gradients and Hessian matrices.  
\begin{enumerate}
\item $\nabla f(\x)$
\item $\nabla^2 f(\x)$
\item $\nabla f(\x)$ when $\a = [1,1,1,1,1]^\top$ and $\x = [0,0,0,0,0]^\top$
\item $\nabla^2 f(\x)$  when $\a = [1,1,1,1,1]^\top$ and $\x = [0,0,0,0,0]^\top$
\end{enumerate}
Note that $0 \le f(\x) \le 1$.

\textbf{Solution:}
\begin{enumerate}
	\item The partial derivative of the logistic function in terms of $x_i$ is $\frac{a_ie^{-\a^T \x}}{(1+e^{-\a^T \x})^2} = a_if(x)(1 - f(x))$, thus the gradient can be described as: 
	$$ \nabla f(\x) = (a_0f(\x)(1-f(\x)), a_1f(\x)(1-f(\x)), ..., a_nf(\x)(1-f(\x)))$$ 
	\item For the second partial derivative, we just take the derivative again in respect for each variable:
	$$ \frac{\partial}{\partial x_j} a_i f(\x)(1 - f(\x)) = a_i[f'(\x)(1-f(\x)) - f(\x)f'(\x)] = a_if'(\x)(1 - 2f(\x))$$
	Since the partial derivative in terms of $x_j$ of $f(\x)$ is really just $a_jf(\x)(1- f(\x))$:
	$$ \frac{\partial^2}{\partial^2 x_i x_j} f(\x) = a_i a_j f(\x)(1 - f(\x))(1 - 2f(\x)) $$
	Meaning that the Hessian matrix can be described as:
	$$ \nabla^2 f(\x) =  \A f(\x)(1-f(\x))(1-2f(\x)) $$
	Where $\A$ is a 2x2 matrix populated by $a_i a_j$ where $i$ and $j$ are the row and column indices.
	\item We see that in this case $\a^T\x = 0$, thus $f(\x) = \frac{e^0}{1 + e^0} = \frac{1}{2}$, and so the partial derivative is $a_i (\frac{1}{2})( \frac{1}{2}) = a_i\frac{1}{4}$, and thus the gradient is:
	$$ \nabla f(\x) = (\frac{1}{4}, \frac{1}{4}, \frac{1}{4}, \frac{1}{4}, \frac{1}{4}) $$
	\item Again $\a^T \x = 0$, so the second partial derivative is $a_i a_j (\frac{1}{2})(\frac{1}{2})(0) = 0$, meaning that:
	$$ \nabla^2 f(\x) = \0 $$
	Where the zero matrix is of size 5x5.
\end{enumerate}

\item~[10 points] Show that $g(x) = -\log(f(\x))$ where $f(\x)$ is a logistic function defined as above, is convex. 

\textbf{Solution:}
We can prove convexity by showing that the second derivative of $g$ is greater than or equal to 0 for all $x$:
$$ g'(x) = -\frac{1}{f(\x)}f'(\x) = -\frac{f(\x)(1 - f(\x))}{f(\x)} = f(\x) - 1 $$
$$ g''(x) = f'(\x) = f(\x)(1 - f(\x)) \geq 0 $$
$$ f(\x) - (f(\x))^2 \geq 0 $$
$$ 1 \geq f(\x) $$
Which is true since $ 0 \leq f(\x) \leq 1 $.

\end{enumerate}


\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
